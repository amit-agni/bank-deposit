{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bank Term Deposits - Data Preparation and Initial Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "### Import Packages and Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   target  target\n0  36548  0.8873 \n1  4640   0.1127 \n   target  target\n0  7310   0.8874 \n1  928    0.1126 \n   target  target\n0  29238  0.8873 \n1  3712   0.1127 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.impute\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,StandardScaler,FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from project_utils import *\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "# %matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "data = pd.read_csv('../data/bank-additional/bank-additional-full.csv',sep=';')\n",
    "\n",
    "dp_dropCols(data,\"duration\")\n",
    "\n",
    "#Convert target to y=1 and n=0 and rename columun\n",
    "data[\"y\"][data[\"y\"] == 'yes'] = 1\n",
    "data[\"y\"][data[\"y\"] == 'no'] = 0\n",
    "data.rename(columns={\"y\":\"target\"}, inplace = True)\n",
    "data[\"target\"] = data[\"target\"].astype('int64')\n",
    "\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2,random_state=42,stratify = data[\"target\"],shuffle = True)\n",
    "\n",
    "#Check proportions \n",
    "countsAndProportions(data[\"target\"])\n",
    "countsAndProportions(test_set[\"target\"])\n",
    "countsAndProportions(train_set[\"target\"])\n",
    "\n",
    "train_set_X = train_set.drop(\"target\", axis=1)\n",
    "train_set_Y = train_set[\"target\"]\n",
    "\n",
    "test_set_X = test_set.drop(\"target\", axis=1)\n",
    "test_set_Y = test_set[\"target\"]\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Create Data prep pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('oe',\n",
       "                                 OrdinalEncoder(categories=[['illiterate',\n",
       "                                                             'basic.4y',\n",
       "                                                             'basic.6y',\n",
       "                                                             'basic.9y',\n",
       "                                                             'high.school',\n",
       "                                                             'professional.course',\n",
       "                                                             'university.degree',\n",
       "                                                             'unknown']]),\n",
       "                                 ['education']),\n",
       "                                ('ohe',\n",
       "                                 OneHotEncoder(handle_unknown='ignore',\n",
       "                                               sparse=False),\n",
       "                                 Index(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
       "       'month', 'day_of_week', 'poutcome'],\n",
       "      dtype='object')),\n",
       "                                ('logAge',\n",
       "                                 FunctionTransformer(func=<ufunc 'log1p'>),\n",
       "                                 ['age']),\n",
       "                                ('scale', StandardScaler(),\n",
       "                                 ['campaign', 'pdays', 'previous',\n",
       "                                  'emp.var.rate', 'cons.price.idx',\n",
       "                                  'cons.conf.idx', 'euribor3m',\n",
       "                                  'nr.employed'])])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "cols_OrdinalEncode = [\"education\"]\n",
    "cols_OHE = data.select_dtypes(include=\"object\").columns\n",
    "cols_log1p = [\"age\"]\n",
    "cols_Numeric = ['campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m','nr.employed']\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "#For OrdinalEncoder, the categories have to be a list of arrays (per column). Since we have only one column\n",
    "('oe', OrdinalEncoder(categories = [['illiterate','basic.4y','basic.6y','basic.9y','high.school','professional.course','university.degree','unknown']]),\n",
    "         cols_OrdinalEncode),\n",
    "('ohe', OneHotEncoder(sparse=False,handle_unknown=\"ignore\"),cols_OHE),\n",
    "('logAge',FunctionTransformer(np.log1p),cols_log1p),\n",
    "('scale', StandardScaler(),cols_Numeric)\n",
    "])\n",
    "\n",
    "#Separate fit and transform, otherwise OHE was giving error as one of the categories were missing in Test\n",
    "full_pipeline.fit(train_set_X)\n",
    "train_prep = full_pipeline.transform(train_set_X)\n",
    "\n",
    "#Apply pipeline to test\n",
    "test_prep = full_pipeline.transform(test_set_X)\n"
   ]
  },
  {
   "source": [
    "### Train few models\n",
    "* Use default hyperparameters\n",
    "* Logistic gives low F1 score and the predicted distribution is also not close to the actuals\n",
    "* Decision Tree is the best model, even though it has low F1\n",
    "* RF and Neural network have higher F1 score but predicted distributions are way off"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================     Logistic Regression     ==================\n",
      "Confusion Matrix :\n",
      " [[7218   92]\n",
      " [ 723  205]]\n",
      "Precision : 0.6902356902356902\n",
      "Recall : 0.2209051724137931\n",
      "F1 Score : 0.3346938775510204\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7941 0.9639\n",
      "1  297  0.0361\n",
      "None\n",
      "==================     Decision Tree     ==================\n",
      "Confusion Matrix :\n",
      " [[6602  708]\n",
      " [ 629  299]]\n",
      "Precision : 0.2969215491559086\n",
      "Recall : 0.32219827586206895\n",
      "F1 Score : 0.3090439276485788\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7231 0.8778\n",
      "1  1007 0.1222\n",
      "None\n",
      "==================     Random Forest     ==================\n",
      "Confusion Matrix :\n",
      " [[7098  212]\n",
      " [ 647  281]]\n",
      "Precision : 0.5699797160243407\n",
      "Recall : 0.30280172413793105\n",
      "F1 Score : 0.39549612948627727\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7745 0.9402\n",
      "1  493  0.0598\n",
      "None\n",
      "==================     Neural Network     ==================\n",
      "Confusion Matrix :\n",
      " [[7035  275]\n",
      " [ 619  309]]\n",
      "Precision : 0.5291095890410958\n",
      "Recall : 0.3329741379310345\n",
      "F1 Score : 0.40873015873015867\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7654 0.9291\n",
      "1  584  0.0709\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"==================     Logistic Regression     ==================\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelTrainPredict(LogisticRegression(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Decision Tree     ==================\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modelTrainPredict(DecisionTreeClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Random Forest     ==================\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelTrainPredict(RandomForestClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Neural Network     ==================\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "modelTrainPredict(MLPClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n"
   ]
  },
  {
   "source": [
    "### Improve the Random Forest Model\n",
    "\n",
    "* Uses random grid search to find the best hyperparameters using 5 fold CV "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=4,\n",
       "                   param_distributions=[{'class_weight': [None, 'balanced',\n",
       "                                                          'balanced_subsample'],\n",
       "                                         'max_depth': [8], 'max_features': [10],\n",
       "                                         'n_estimators': [100, 500]}],\n",
       "                   return_train_score=True, scoring='f1')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [{'n_estimators': [100, 500]\n",
    "                ,'max_features': [10]\n",
    "                ,'max_depth':[8]\n",
    "                ,'class_weight':[None,'balanced','balanced_subsample']\n",
    "                }\n",
    "                # ,{'n_estimators': [100, 200,300,400, 500,1000]\n",
    "                # ,'max_features': ['auto','sqrt','log2',6,8,10]\n",
    "                # ,'max_depth':[2,3,4,6,8]\n",
    "                # ,'class_weight':[None,'balanced','balanced_subsample']                }\n",
    "                #,{'bootstrap': [False], 'n_estimators': [100, 500], 'max_features': [2, 3, 4]}\n",
    "                ,]\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(),param_grid, cv=5,scoring='f1',return_train_score=True, n_iter=4)\n",
    "random_search.fit(train_prep,train_set_Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Hyperparameter search results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean_fit_time  mean_test_score  \\\n",
       "2 8.1867         0.4743             \n",
       "3 2.0952         0.4735             \n",
       "0 1.6671         0.4724             \n",
       "1 8.4359         0.3244             \n",
       "\n",
       "                                                                                            params  \n",
       "2  {'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}            \n",
       "3  {'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced_subsample'}  \n",
       "0  {'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}            \n",
       "1  {'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': None}                  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>mean_test_score</th>\n      <th>params</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>8.1867</td>\n      <td>0.4743</td>\n      <td>{'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0952</td>\n      <td>0.4735</td>\n      <td>{'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced_subsample'}</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1.6671</td>\n      <td>0.4724</td>\n      <td>{'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.4359</td>\n      <td>0.3244</td>\n      <td>{'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': None}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "cvres = random_search.cv_results_\n",
    "#cvres.keys()\n",
    "out = pd.DataFrame(data = {\"mean_fit_time\":cvres[\"mean_fit_time\"],\"mean_test_score\":cvres[\"mean_test_score\"],\"params\":cvres[\"params\"]})\n",
    "out.sort_values([\"mean_test_score\"],ascending=False)\n"
   ]
  },
  {
   "source": [
    "### Predict on Test using the best estimator\n",
    "\n",
    "* Better than the default parameters\n",
    "* Higher F1 score but performance is not better than Decision Tree \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=8, max_features=10,\n",
       "                       n_estimators=500)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix :\n [[6449  861]\n [ 337  591]]\nPrecision : 0.40702479338842973\nRecall : 0.6368534482758621\nF1 Score : 0.4966386554621849\n.............................\ny_true Counts :\n   target  target\n0  7310   0.8874 \n1  928    0.1126 \nNone\npred Counts :\n      0      1\n0  6786 0.8237\n1  1452 0.1763\nNone\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier(class_weight='balanced', max_depth=8, max_features=10,n_estimators=500)\n",
    "#random_search.best_index_\n",
    "random_search.best_estimator_\n",
    "\n",
    "pred = random_search.predict(test_prep)\n",
    "classificationMetrics(test_set_Y,pred)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "source": [
    "## so far\n",
    "\n",
    "1. Fit pipeline to train set and transform it, so that it can be fed to the models\n",
    "2. (Tried various models - Go deeper to see why RF Classifier is not giving the best results )\n",
    "3. Randomised Grid Search Cross Validation\n",
    "\n",
    "## TO DO \n",
    "1. Create own metric to get higher values for True Positives\n",
    "2. Discretize continuous\n",
    "3. Treat some of the data prep steps as hyperparameters\n",
    "4. Feature importance\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "Transformer oe (type OrdinalEncoder) does not provide get_feature_names.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2bf6f462b463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# random_search.best_estimator_.feature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfull_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 raise AttributeError(\"Transformer %s (type %s) does not \"\n\u001b[0m\u001b[1;32m    372\u001b[0m                                      \u001b[0;34m\"provide get_feature_names.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                                      % (str(name), type(trans).__name__))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Transformer oe (type OrdinalEncoder) does not provide get_feature_names."
     ]
    }
   ],
   "source": [
    "x = full_pipeline.named_transformers_[\"ohe\"]\n",
    "# list(x.categories_)\n",
    "# random_search.best_estimator_.feature_importances_\n",
    "\n",
    "full_pipeline.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #avoid cell execution\n",
    "\n",
    "### Cross Validation - Example\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "#sorted(sklearn.metrics.SCORERS.keys())\n",
    "scores = cross_val_score(DecisionTreeClassifier(), train_prep, train_set_Y,scoring=\"f1\", cv=10)\n",
    "scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #avoid cell execution\n",
    "\n",
    "#The imputer stores the median values in **statistics_** instance variable. We cannot be sure that there wonâ€™t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes\n",
    "data = dp_ImputeNumericCols(data)\n",
    "\n",
    "#Encode education as ordinal encoder and see if its inherent ordering property would lead to better results\n",
    "cols = [\"education\"]\n",
    "data = dp_EncodeOrdinalCols(data,cols,categories = [['illiterate','basic.4y','basic.6y','basic.9y','high.school','professional.course','university.degree','unknown']])\n",
    "\n",
    "cols = data.select_dtypes(include=\"object\").columns\n",
    "enc = DataframeOneHotEncoder(cols)\n",
    "data = enc.transform(data)    \n",
    "\n",
    "standardiseCols = ['age', 'campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m','nr.employed']\n",
    "data = dp_StandardiseNumericCols(data,standardiseCols)\n"
   ]
  }
 ]
}