{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.6 64-bit",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Bank Term Deposits - Data Preparation and Initial Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "source": [
    "### Import Packages and Load Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   target  target\n0  36548  0.8873 \n1  4640   0.1127 \n   target  target\n0  7310   0.8874 \n1  928    0.1126 \n   target  target\n0  29238  0.8873 \n1  3712   0.1127 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.impute\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder,StandardScaler,FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from project_utils import *\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "# %matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "data = pd.read_csv('../data/bank-additional/bank-additional-full.csv',sep=';')\n",
    "\n",
    "dp_dropCols(data,\"duration\")\n",
    "\n",
    "#Convert target to y=1 and n=0 and rename columun\n",
    "data[\"y\"][data[\"y\"] == 'yes'] = 1\n",
    "data[\"y\"][data[\"y\"] == 'no'] = 0\n",
    "data.rename(columns={\"y\":\"target\"}, inplace = True)\n",
    "data[\"target\"] = data[\"target\"].astype('int64')\n",
    "\n",
    "\n",
    "train_set, test_set = train_test_split(data, test_size=0.2,random_state=42,stratify = data[\"target\"],shuffle = True)\n",
    "\n",
    "#Check proportions \n",
    "countsAndProportions(data[\"target\"])\n",
    "countsAndProportions(test_set[\"target\"])\n",
    "countsAndProportions(train_set[\"target\"])\n",
    "\n",
    "train_set_X = train_set.drop(\"target\", axis=1)\n",
    "train_set_Y = train_set[\"target\"]\n",
    "\n",
    "test_set_X = test_set.drop(\"target\", axis=1)\n",
    "test_set_Y = test_set[\"target\"]\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Create Data prep pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('oe',\n",
       "                                 OrdinalEncoder(categories=[['illiterate',\n",
       "                                                             'basic.4y',\n",
       "                                                             'basic.6y',\n",
       "                                                             'basic.9y',\n",
       "                                                             'high.school',\n",
       "                                                             'professional.course',\n",
       "                                                             'university.degree',\n",
       "                                                             'unknown']]),\n",
       "                                 ['education']),\n",
       "                                ('ohe',\n",
       "                                 OneHotEncoder(handle_unknown='ignore',\n",
       "                                               sparse=False),\n",
       "                                 Index(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
       "       'month', 'day_of_week', 'poutcome'],\n",
       "      dtype='object')),\n",
       "                                ('logAge',\n",
       "                                 FunctionTransformer(func=<ufunc 'log1p'>),\n",
       "                                 ['age']),\n",
       "                                ('scale', StandardScaler(),\n",
       "                                 ['campaign', 'pdays', 'previous',\n",
       "                                  'emp.var.rate', 'cons.price.idx',\n",
       "                                  'cons.conf.idx', 'euribor3m',\n",
       "                                  'nr.employed'])])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "cols_OrdinalEncode = [\"education\"]\n",
    "cols_OHE = data.select_dtypes(include=\"object\").columns\n",
    "cols_log1p = [\"age\"]\n",
    "cols_Numeric = ['campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m','nr.employed']\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "#For OrdinalEncoder, the categories have to be a list of arrays (per column). Since we have only one column\n",
    "('oe', OrdinalEncoder(categories = [['illiterate','basic.4y','basic.6y','basic.9y','high.school','professional.course','university.degree','unknown']]),\n",
    "         cols_OrdinalEncode),\n",
    "('ohe', OneHotEncoder(sparse=False,handle_unknown=\"ignore\"),cols_OHE),\n",
    "('logAge',FunctionTransformer(np.log1p),cols_log1p),\n",
    "('scale', StandardScaler(),cols_Numeric)\n",
    "])\n",
    "\n",
    "#Separate fit and transform, otherwise OHE was giving error as one of the categories were missing in Test\n",
    "full_pipeline.fit(train_set_X)\n",
    "train_prep = full_pipeline.transform(train_set_X)\n",
    "\n",
    "#Apply pipeline to test\n",
    "test_prep = full_pipeline.transform(test_set_X)\n"
   ]
  },
  {
   "source": [
    "### Train few models\n",
    "* Use default hyperparameters\n",
    "* Logistic gives low F1 score and the predicted distribution is also not close to the actuals\n",
    "* Decision Tree is the best model, even though it has low F1\n",
    "* RF and Neural network have higher F1 score but predicted distributions are way off"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================     Logistic Regression     ==================\n",
      "Confusion Matrix :\n",
      " [[7218   92]\n",
      " [ 723  205]]\n",
      "Precision : 0.6902356902356902\n",
      "Recall : 0.2209051724137931\n",
      "F1 Score : 0.3346938775510204\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7941 0.9639\n",
      "1  297  0.0361\n",
      "None\n",
      "==================     Decision Tree     ==================\n",
      "Confusion Matrix :\n",
      " [[6602  708]\n",
      " [ 629  299]]\n",
      "Precision : 0.2969215491559086\n",
      "Recall : 0.32219827586206895\n",
      "F1 Score : 0.3090439276485788\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7231 0.8778\n",
      "1  1007 0.1222\n",
      "None\n",
      "==================     Random Forest     ==================\n",
      "Confusion Matrix :\n",
      " [[7098  212]\n",
      " [ 647  281]]\n",
      "Precision : 0.5699797160243407\n",
      "Recall : 0.30280172413793105\n",
      "F1 Score : 0.39549612948627727\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7745 0.9402\n",
      "1  493  0.0598\n",
      "None\n",
      "==================     Neural Network     ==================\n",
      "Confusion Matrix :\n",
      " [[7035  275]\n",
      " [ 619  309]]\n",
      "Precision : 0.5291095890410958\n",
      "Recall : 0.3329741379310345\n",
      "F1 Score : 0.40873015873015867\n",
      ".............................\n",
      "y_true Counts :\n",
      "   target  target\n",
      "0  7310   0.8874 \n",
      "1  928    0.1126 \n",
      "None\n",
      "pred Counts :\n",
      "      0      1\n",
      "0  7654 0.9291\n",
      "1  584  0.0709\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"==================     Logistic Regression     ==================\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelTrainPredict(LogisticRegression(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Decision Tree     ==================\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modelTrainPredict(DecisionTreeClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Random Forest     ==================\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "modelTrainPredict(RandomForestClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n",
    "\n",
    "print(\"==================     Neural Network     ==================\")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "modelTrainPredict(MLPClassifier(),train_prep,train_set_Y,test_prep,test_set_Y)\n"
   ]
  },
  {
   "source": [
    "### Improve the Random Forest Model\n",
    "\n",
    "* Uses random grid search to find the best hyperparameters using 5 fold CV "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_iter=4,\n",
       "                   param_distributions=[{'class_weight': [None, 'balanced',\n",
       "                                                          'balanced_subsample'],\n",
       "                                         'max_depth': [8], 'max_features': [10],\n",
       "                                         'n_estimators': [100, 500]}],\n",
       "                   return_train_score=True, scoring='f1')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = [{'n_estimators': [100, 500]\n",
    "                ,'max_features': [10]\n",
    "                ,'max_depth':[8]\n",
    "                ,'class_weight':[None,'balanced','balanced_subsample']\n",
    "                }\n",
    "                # ,{'n_estimators': [100, 200,300,400, 500,1000]\n",
    "                # ,'max_features': ['auto','sqrt','log2',6,8,10]\n",
    "                # ,'max_depth':[2,3,4,6,8]\n",
    "                # ,'class_weight':[None,'balanced','balanced_subsample']                }\n",
    "                #,{'bootstrap': [False], 'n_estimators': [100, 500], 'max_features': [2, 3, 4]}\n",
    "                ,]\n",
    "\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(),param_grid, cv=5,scoring='f1',return_train_score=True, n_iter=4)\n",
    "random_search.fit(train_prep,train_set_Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Hyperparameter search results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   mean_fit_time  mean_test_score  \\\n",
       "2 8.1867         0.4743             \n",
       "3 2.0952         0.4735             \n",
       "0 1.6671         0.4724             \n",
       "1 8.4359         0.3244             \n",
       "\n",
       "                                                                                            params  \n",
       "2  {'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}            \n",
       "3  {'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced_subsample'}  \n",
       "0  {'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}            \n",
       "1  {'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': None}                  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>mean_test_score</th>\n      <th>params</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>8.1867</td>\n      <td>0.4743</td>\n      <td>{'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0952</td>\n      <td>0.4735</td>\n      <td>{'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced_subsample'}</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1.6671</td>\n      <td>0.4724</td>\n      <td>{'n_estimators': 100, 'max_features': 10, 'max_depth': 8, 'class_weight': 'balanced'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.4359</td>\n      <td>0.3244</td>\n      <td>{'n_estimators': 500, 'max_features': 10, 'max_depth': 8, 'class_weight': None}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "cvres = random_search.cv_results_\n",
    "#cvres.keys()\n",
    "out = pd.DataFrame(data = {\"mean_fit_time\":cvres[\"mean_fit_time\"],\"mean_test_score\":cvres[\"mean_test_score\"],\"params\":cvres[\"params\"]})\n",
    "out.sort_values([\"mean_test_score\"],ascending=False)\n"
   ]
  },
  {
   "source": [
    "### Predict on Test using the best estimator\n",
    "\n",
    "* Better than the default parameters\n",
    "* Higher F1 score but performance is not better than Decision Tree \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=8, max_features=10,\n",
       "                       n_estimators=500)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix :\n [[6449  861]\n [ 337  591]]\nPrecision : 0.40702479338842973\nRecall : 0.6368534482758621\nF1 Score : 0.4966386554621849\n.............................\ny_true Counts :\n   target  target\n0  7310   0.8874 \n1  928    0.1126 \nNone\npred Counts :\n      0      1\n0  6786 0.8237\n1  1452 0.1763\nNone\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier(class_weight='balanced', max_depth=8, max_features=10,n_estimators=500)\n",
    "#random_search.best_index_\n",
    "random_search.best_estimator_\n",
    "\n",
    "pred = random_search.predict(test_prep)\n",
    "classificationMetrics(test_set_Y,pred)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "source": [
    "## so far\n",
    "\n",
    "1. Fit pipeline to train set and transform it, so that it can be fed to the models\n",
    "2. (Tried various models - Go deeper to see why RF Classifier is not giving the best results )\n",
    "3. Randomised Grid Search Cross Validation\n",
    "\n",
    "## TO DO \n",
    "1. Create own metric to get higher values for True Positives\n",
    "2. Discretize continuous\n",
    "3. Treat some of the data prep steps as hyperparameters\n",
    "4. Feature importance\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "Transformer oe (type OrdinalEncoder) does not provide get_feature_names.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2bf6f462b463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# random_search.best_estimator_.feature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfull_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get_feature_names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                 raise AttributeError(\"Transformer %s (type %s) does not \"\n\u001b[0m\u001b[1;32m    372\u001b[0m                                      \u001b[0;34m\"provide get_feature_names.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                                      % (str(name), type(trans).__name__))\n",
      "\u001b[0;31mAttributeError\u001b[0m: Transformer oe (type OrdinalEncoder) does not provide get_feature_names."
     ]
    }
   ],
   "source": [
    "x = full_pipeline.named_transformers_[\"ohe\"]\n",
    "# list(x.categories_)\n",
    "# random_search.best_estimator_.feature_importances_\n",
    "\n",
    "full_pipeline.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #avoid cell execution\n",
    "\n",
    "### Cross Validation - Example\n",
    " \n",
    "from sklearn.model_selection import cross_val_score\n",
    "#sorted(sklearn.metrics.SCORERS.keys())\n",
    "scores = cross_val_score(DecisionTreeClassifier(), train_prep, train_set_Y,scoring=\"f1\", cv=10)\n",
    "scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error #avoid cell execution\n",
    "\n",
    "#The imputer stores the median values in **statistics_** instance variable. We cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes\n",
    "data = dp_ImputeNumericCols(data)\n",
    "\n",
    "#Encode education as ordinal encoder and see if its inherent ordering property would lead to better results\n",
    "cols = [\"education\"]\n",
    "data = dp_EncodeOrdinalCols(data,cols,categories = [['illiterate','basic.4y','basic.6y','basic.9y','high.school','professional.course','university.degree','unknown']])\n",
    "\n",
    "cols = data.select_dtypes(include=\"object\").columns\n",
    "enc = DataframeOneHotEncoder(cols)\n",
    "data = enc.transform(data)    \n",
    "\n",
    "standardiseCols = ['age', 'campaign', 'pdays', 'previous','emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m','nr.employed']\n",
    "data = dp_StandardiseNumericCols(data,standardiseCols)\n"
   ]
  }
 ]
}